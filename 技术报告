人脸识别技术报告
熊荐辕 201700171089

此代码主体思路：
  首先使用CascadeClassifier分类器提取了每张图片的脸部部分并将其和标签一起保存起来，其中每一个人为一个标签，将图像按7比3的比例分为训练集
和测试集，将脸部部分图像用pca降维后提取其主要特征，随后用k近邻法进行投票选出图片标签。用测试图片的标签与图片预测的标签进行比较得到其精确度。

一、CascadeClassifier
在OpenCV中，自带着Harr分类器人脸特征训练的文件，利用这些文件，我们可以很方面的进行人脸，眼睛，鼻子，表情等的检测。
我们使用此分类器提取出每幅图片中的人脸部分，并将同一个人的人脸图片打上同一个标签，将其和其标签一一对应并保存起来。

二、pca：
（1）基本原理：
 主成分分析是一种矩阵的压缩算法，在减少矩阵维数的同时尽可能的保留原矩阵的信息，简单来说就是将 n×m的矩阵转换成n×k的矩阵，仅保留矩阵中所
存在的主要特性，从而可以大大节省空间和数据量。

（2）主要步骤：
设有m条n维数据。
1）将原始数据按列组成n行m列矩阵X
2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值
3）求出协方差矩阵
4）求出协方差矩阵的特征值及对应的特征向量
5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
6）此即为降维到k维后的数据

（3）在此代码中的应用：
对前面提取出的人脸图片进行pca降维，前面提取出的人脸灰度图片的像素值即为即为一个n行m列的矩阵，用pca提取出其k行值，这些值作为矩阵的主要
特征可以作为区分人脸的主要依据。

三、KNN
（1）基本原理：
所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居）， 这K个实例
的多数属于某个类，就把该输入实例分类到这个类中。

（2）基本步骤：
1、计算测试对象到训练集中每个对象的距离
2、按照距离的远近排序
3、选取与当前测试对象最近的k的训练对象，作为该测试对象的邻居
4、统计这k个邻居的类别频率
5、k个邻居里频率最高的类别，即为测试对象的类别

（3）在此代码中的应用：
我们将所有训练图片的pca特征与需要预测的测试图片的pca特征进行比对并计算他们的欧式距离，选出与测试图片pca特征欧式距离最小的k个训练图片进
行投票得到训练数据的预测标签。

（4）KNN算法的优缺点：
KNN的主要优点有：
1） 理论成熟，思想简单，既可以用来做分类也可以用来做回归
2） 可用于非线性分类
3） 训练时间复杂度比支持向量机之类的算法低，仅为O(n)
4） 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感
5） 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方
法较其他方法更为适合
6）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分

KNN的主要缺点有：
1）计算量大，尤其是特征数非常多的时候
2）样本不平衡的时候，对稀有类别的预测准确率低
3）KD树，球树之类的模型建立需要大量的内存
4）使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢
5）相比决策树模型，KNN模型可解释性不强